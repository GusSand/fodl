---
layout: page
title: Introduction
categories: intro
date: 2022-01-19
---

Our goal is to learn to reason about (deep) neural networks from the lens of *theory*.

Unlike other scientific fields, there is currently a *very* wide gap between what the best available tools in theoretical computer science can tell us about modern neural networks, and the actual ways in which they are deployed. A major motivation behind these course notes is to identify the landscape of how wide exactly these gaps are at present. By asking carefully crafted (but precise) questions, the hope is that one can shed light on why certain aspects of neural networks work (or don't).

This is by no means the first attempt to do so. Other excellent courses/lecture notes include:

* [Fit without Fear](https://arxiv.org/pdf/2105.14368.pdf) by Misha Belkin (UCSD).
* [Mathematics of Deep Learning](https://www.notion.so/Mathematics-of-Deep-Learning-05cd9255f03842489083ec7cbb6338d5) by Joan Bruna (NYU).
* [Deep Learning Theory](https://mjt.cs.illinois.edu/dlt/) by Matus Telgarsky (UIUC; I particularly like these).

## Structure
{:.label}

In order to best organize

### Lagrange's Theorem
{:.label}
